{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a023a0-9ab5-49b5-86bc-a683c938bb91",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45b0adbd-1d2b-4f41-a1da-3b1cd0138f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from typing import Optional, Union\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15f1d719-0a22-40af-855d-fcd48ad90637",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"training.csv\")\n",
    "test_df = pd.read_csv(\"testset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4574ec5b-703e-45bc-a882-14d2ec1c1bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>movieid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158862</td>\n",
       "      <td>1600</td>\n",
       "      <td>4.0</td>\n",
       "      <td>978307288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49693</td>\n",
       "      <td>9102</td>\n",
       "      <td>4.0</td>\n",
       "      <td>985466347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65237</td>\n",
       "      <td>2408</td>\n",
       "      <td>3.0</td>\n",
       "      <td>978308050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82765</td>\n",
       "      <td>2590</td>\n",
       "      <td>2.0</td>\n",
       "      <td>985139361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120946</td>\n",
       "      <td>285</td>\n",
       "      <td>2.0</td>\n",
       "      <td>978308465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  movieid  rating  timestamp\n",
       "0  158862     1600     4.0  978307288\n",
       "1   49693     9102     4.0  985466347\n",
       "2   65237     2408     3.0  978308050\n",
       "3   82765     2590     2.0  985139361\n",
       "4  120946      285     2.0  978308465"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39147c51-158e-4f6b-b2c1-be957e23b85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>movieid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.178277e+07</td>\n",
       "      <td>1.178277e+07</td>\n",
       "      <td>1.178277e+07</td>\n",
       "      <td>1.178277e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.419319e+05</td>\n",
       "      <td>1.919155e+03</td>\n",
       "      <td>3.502378e+00</td>\n",
       "      <td>1.158040e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.179844e+04</td>\n",
       "      <td>2.822795e+03</td>\n",
       "      <td>1.059831e+00</td>\n",
       "      <td>1.034930e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>9.783073e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.142000e+04</td>\n",
       "      <td>3.700000e+02</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.077814e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.421970e+05</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>3.500000e+00</td>\n",
       "      <td>1.144422e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.130310e+05</td>\n",
       "      <td>2.113000e+03</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.236016e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.832270e+05</td>\n",
       "      <td>5.385900e+04</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.380320e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userid       movieid        rating     timestamp\n",
       "count  1.178277e+07  1.178277e+07  1.178277e+07  1.178277e+07\n",
       "mean   1.419319e+05  1.919155e+03  3.502378e+00  1.158040e+09\n",
       "std    8.179844e+04  2.822795e+03  1.059831e+00  1.034930e+08\n",
       "min    0.000000e+00  0.000000e+00  5.000000e-01  9.783073e+08\n",
       "25%    7.142000e+04  3.700000e+02  3.000000e+00  1.077814e+09\n",
       "50%    1.421970e+05  8.910000e+02  3.500000e+00  1.144422e+09\n",
       "75%    2.130310e+05  2.113000e+03  4.000000e+00  1.236016e+09\n",
       "max    2.832270e+05  5.385900e+04  5.000000e+00  1.380320e+09"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "801e363c-787c-4863-8790-102d3b2c9e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users in test set: 2963\n",
      "Users present in training set: 0\n",
      "Users NOT present in training set: 2963\n"
     ]
    }
   ],
   "source": [
    "train_users = set(train_df['userid'].unique())\n",
    "test_users = set(test_df['userid'].unique())\n",
    "\n",
    "users_in_train = test_users.intersection(train_users)\n",
    "users_not_in_train = test_users.difference(train_users)\n",
    "\n",
    "print(f\"Total users in test set: {len(test_users)}\")\n",
    "print(f\"Users present in training set: {len(users_in_train)}\")\n",
    "print(f\"Users NOT present in training set: {len(users_not_in_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28477d87-ab6f-434b-b2d4-62f11141887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of users in train that have seen top 4500 most watched films: 98.25%\n",
      "Percentage of users in train that have seen top 4500 rated films: 61.13%\n"
     ]
    }
   ],
   "source": [
    "top_popular_movies_count_train = (\n",
    "    train_df.groupby('movieid')\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(7250)\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "top_popular_movies_rating_train = (\n",
    "    train_df.groupby('movieid')['rating']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(7250)\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "movie_counts = train_df['movieid'].value_counts() \n",
    "total_interactions = len(train_df) \n",
    "\n",
    "popular_count = sum(movie_counts.get(movie, 0) for movie in top_popular_movies_count_train)\n",
    "popular_rating = sum(movie_counts.get(movie, 0) for movie in top_popular_movies_rating_train)\n",
    "\n",
    "percentage_count = (popular_count / total_interactions) * 100\n",
    "percentage_rating = (popular_rating / total_interactions) * 100\n",
    "\n",
    "print(f\"Percentage of users in train that have seen top 4500 most watched films: {percentage_count:.2f}%\")\n",
    "print(f\"Percentage of users in train that have seen top 4500 rated films: {percentage_rating:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c7020-0ee9-4922-8313-604ffe5d6ec0",
   "metadata": {},
   "source": [
    "As wee see almost 98% of all movies can be explained by 7250 most watched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1272e497-9fa2-4826-8982-978057f5eaa7",
   "metadata": {},
   "source": [
    "### Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f0451859-57bf-4bc1-8f4f-80da69cbdfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timepoint_split(data, time_split_q=0.95):\n",
    "    \"\"\"\n",
    "    Split data into training, testset, and holdout datasets based on a timepoint split\n",
    "    and according to the `warm-start` evaluation strategy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The input dataset containing columns `userid`, `movieid`, and `timestamp`.\n",
    "    time_split_q : float, optional\n",
    "        The quantile value used to split the dataset based on the `timestamp` column.\n",
    "        Default is 0.95.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
    "        A tuple of three pandas DataFrames: training, testset, and holdout.\n",
    "        `training` is a subset of `data` used for training the recommender system.\n",
    "        `testset` is a subset of `data` used for generating recommendations for the test users.\n",
    "        `holdout` is a subset excluded from `testset` containing only the most recent interactions for each test user.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function splits the input `data` into three subsets: `training`, `testset`, and `holdout`.\n",
    "    The split is performed based on the `timestamp` column of `data`, using `time_split_q` as the quantile value.\n",
    "    The `holdout` dataset contains only the immediate interactions following the fixed timepoint for each test user from the `testset`.\n",
    "    The set of users in `training` is disjoint with the set of users in the `testset`, which implements the `warm-start` scenario.\n",
    "    \"\"\"    \n",
    "    timepoint = data.timestamp.quantile(q=time_split_q, interpolation='nearest')\n",
    "    test_ = data.query('timestamp >= @timepoint')\n",
    "    rest_ = data.drop(test_.index)\n",
    "    holdout_ = (\n",
    "        test_\n",
    "        .sort_values('timestamp')\n",
    "        .drop_duplicates(subset=['userid'], keep='first')\n",
    "    )\n",
    "    # the holdout dataframe contains interactions closest to certain timepoint from the right,\n",
    "    # i.e., the corresponding items are the first in each test user profile after this timepoint\n",
    "    training = rest_.query('userid not in @holdout_.userid')\n",
    "    train_items = training.movieid.unique()\n",
    "    testset_ = rest_.query('userid in @holdout_.userid and movieid in @train_items')\n",
    "    test_users = testset_.userid.unique()\n",
    "    holdout = holdout_.query(\n",
    "        # if user is not in `test_users` then no evluation is possible,\n",
    "        # if item is not in `train_items` it's cold start -> must be excluded\n",
    "        'userid in @test_users and movieid in @train_items'\n",
    "    ).sort_values('userid')\n",
    "    testset = testset_.query(\n",
    "        # make sure testset and holdout contain the same set of users\n",
    "        'userid in @holdout.userid'\n",
    "    ).sort_values('userid')\n",
    "    return training, testset, holdout\n",
    "\n",
    "def leave_last_out(data, userid='userid', timeid='timestamp'):\n",
    "    data_sorted = data.sort_values('timestamp')\n",
    "    holdout = data_sorted.drop_duplicates(\n",
    "        subset=['userid'], keep='last'\n",
    "    ) # split the last item from each user's history\n",
    "    remaining = data.drop(holdout.index) # store the remaining data - will be our training\n",
    "    return remaining, holdout\n",
    "\n",
    "def transform_indices(data: pd.DataFrame, users: str, items:str, inplace: bool=False):\n",
    "    '''\n",
    "    Reindex columns that correspond to users and items.\n",
    "    New index is contiguous starting from 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The input data to be reindexed.\n",
    "    users : str\n",
    "        The name of the column in `data` that contains user IDs.\n",
    "    items : str\n",
    "        The name of the column in `data` that contains item IDs.\n",
    "    inplace : bool\n",
    "        whether the data should be modified inplace, `False` by default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame, dict\n",
    "        The reindexed data and a dictionary with mapping between original IDs and the new numeric IDs.\n",
    "        The keys of the dictionary are 'users' and 'items'.\n",
    "        The values of the dictionary are pandas Index objects.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> data = pd.DataFrame({'customers': ['A', 'B', 'C'], 'products': ['X', 'Y', 'Z'], 'rating': [1, 2, 3]})\n",
    "    >>> data_reindexed, data_index = transform_indices(data, 'customers', 'products')\n",
    "    >>> data_reindexed\n",
    "       users  items  rating\n",
    "    0      0      0       1\n",
    "    1      1      1       2\n",
    "    2      2      2       3\n",
    "    >>> data_index\n",
    "    {\n",
    "        'users': Index(['A', 'B', 'C'], dtype='object', name='customers'),\n",
    "        'items': Index(['X', 'Y', 'Z'], dtype='object', name='products')\n",
    "    }\n",
    "    '''\n",
    "    data_index = {}\n",
    "    data_codes = {}\n",
    "    for entity, field in zip(['users', 'items'], [users, items]):\n",
    "        new_index, data_index[entity] = to_numeric_id(data, field)\n",
    "        if inplace:\n",
    "            data.loc[:, field] = new_index\n",
    "        else:\n",
    "            data_codes[field] = new_index\n",
    "\n",
    "    if data_codes:\n",
    "        data = data.assign(**data_codes) # makes a copy of data\n",
    "    return data, data_index\n",
    "\n",
    "\n",
    "def to_numeric_id(data: pd.DataFrame, field: str):\n",
    "    \"\"\"\n",
    "    This function takes in two arguments, data and field. It converts the data field\n",
    "    into categorical values and creates a new contiguous index. It then creates an\n",
    "    idx_map which is a renamed version of the field argument. Finally, it returns the\n",
    "    idx and idx_map variables.\n",
    "    \"\"\"\n",
    "    idx_data = data[field].astype(\"category\")\n",
    "    idx = idx_data.cat.codes\n",
    "    idx_map = idx_data.cat.categories.rename(field)\n",
    "    return idx, idx_map\n",
    "\n",
    "\n",
    "def reindex_data(\n",
    "        data: pd.DataFrame,\n",
    "        data_index: dict,\n",
    "        entities: Optional[Union[str, list[str]]] = None,\n",
    "        filter_invalid: bool = True,\n",
    "        inplace: bool = False\n",
    "    ):\n",
    "    '''\n",
    "    Reindex provided data with the specified index mapping.\n",
    "    By default, will take the name of the fields to reindex from `data_index`.\n",
    "    It is also possible to specify which field to reindex by providing `entities`.\n",
    "    '''\n",
    "    if entities is None:\n",
    "        entities = data_index.keys()\n",
    "    if isinstance(entities, str): # handle single entity provided as a string\n",
    "        entities = [entities]\n",
    "\n",
    "    data_codes = {}\n",
    "    for entity in entities:\n",
    "        entity_index = data_index[entity]\n",
    "        field = entity_index.name # extract the field name\n",
    "        new_index = entity_index.get_indexer(data[field])\n",
    "        if inplace:\n",
    "            data.loc[:, field] = new_index # assign new values inplace\n",
    "        else:\n",
    "            data_codes[field] = new_index # store new values\n",
    "\n",
    "    if data_codes:\n",
    "        data = data.assign(**data_codes) # assign new values by making a copy\n",
    "\n",
    "    if filter_invalid: # discard unrecognized entity index\n",
    "        valid_values = [f'{data_index[entity].name}>=0' for entity in entities]\n",
    "        data = data.query(' and '.join(valid_values))\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_interactions_matrix(\n",
    "        data: pd.DataFrame,\n",
    "        data_description: dict,\n",
    "        rebase_users: bool=False\n",
    "    ):\n",
    "    '''\n",
    "    Converts a pandas dataframe with user-item interactions into a sparse matrix representation.\n",
    "    Allows reindexing user ids, which help ensure data consistency at the scoring stage\n",
    "    (assumes user ids are sorted in the scoring array).\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input dataframe containing the user-item interactions.\n",
    "        data_description (dict): A dictionary containing the data description with the following keys:\n",
    "            - 'n_users' (int): The total number of unique users in the data.\n",
    "            - 'n_items' (int): The total number of unique items in the data.\n",
    "            - 'users' (str): The name of the column in the dataframe containing the user ids.\n",
    "            - 'items' (str): The name of the column in the dataframe containing the item ids.\n",
    "            - 'feedback' (str): The name of the column in the dataframe containing the user-item interaction feedback.\n",
    "        rebase_users (bool, optional): Whether to reindex the user ids to make contiguous index starting from 0. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: A sparse matrix of shape (n_users, n_items) containing the user-item interactions.\n",
    "    '''\n",
    "\n",
    "    n_users = data_description['n_users']\n",
    "    n_items = data_description['n_items']\n",
    "    # get indices of observed data\n",
    "    user_idx = data[data_description['users']].values\n",
    "    if rebase_users: # handle non-contiguous index of test users\n",
    "        # This ensures that all user ids are contiguous and start from 0,\n",
    "        # which helps ensure data consistency at the scoring stage.\n",
    "        user_idx, user_index = pd.factorize(user_idx, sort=True)\n",
    "        n_users = len(user_index)\n",
    "    item_idx = data[data_description['items']].values\n",
    "    feedback = data[data_description['feedback']].values\n",
    "    # construct rating matrix\n",
    "    return csr_matrix((feedback, (user_idx, item_idx)), shape=(n_users, n_items))\n",
    "\n",
    "\n",
    "def verify_time_split(\n",
    "        before: pd.DataFrame,\n",
    "        after: pd.DataFrame,\n",
    "        target_field: str='userid',\n",
    "        timeid: str='timestamp'\n",
    "    ):\n",
    "    '''\n",
    "    Check that items from `after` dataframe have later timestamps than\n",
    "    any corresponding item from the `before` dataframe. Compare w.r.t target_field.\n",
    "    Usage example: assert that for any user, the holdout items are the most recent ones.\n",
    "    '''\n",
    "    before_ts = before.groupby(target_field)[timeid].max()\n",
    "    after_ts = after.groupby(target_field)[timeid].min()\n",
    "    assert (\n",
    "        before_ts\n",
    "        .reindex(after_ts.index)\n",
    "        .combine(after_ts, lambda x, y: True if x!=x else x <= y)\n",
    "    ).all()\n",
    "\n",
    "def downvote_seen_items(scores: np.ndarray, data: pd.DataFrame, data_description: dict) -> None:\n",
    "    \"\"\"\n",
    "    Downvote relevance scores for seen items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores : np.ndarray\n",
    "        A dense numpy array of scores.\n",
    "    data : pd.DataFrame\n",
    "        A pandas DataFrame containing user-item interaction data.\n",
    "    data_description : dict\n",
    "        A dictionary containing metadata about the data, including 'items' and 'users' keys.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function modifies the input scores array in-place.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function assumes that the input scores array is sorted by user index.\n",
    "    It also assumes that the `users` and `items` keys in the `data_description` dictionary\n",
    "    correspond to the column names in the `data` DataFrame.\n",
    "    \"\"\"\n",
    "    assert isinstance(scores, np.ndarray), 'Scores must be a dense numpy array!'\n",
    "    itemid = data_description['items']\n",
    "    userid = data_description['users']\n",
    "    # get indices of observed data, corresponding to scores array\n",
    "    # we need to provide correct mapping of rows in scores array into\n",
    "    # the corresponding user index (which is assumed to be sorted)\n",
    "    row_idx, test_users = pd.factorize(data[userid], sort=True)\n",
    "    assert len(test_users) == scores.shape[0]\n",
    "    col_idx = data[itemid].values\n",
    "    # downvote scores at the corresponding positions\n",
    "    scores[row_idx, col_idx] = scores.min() - 1\n",
    "\n",
    "\n",
    "def topn_recommendations(scores: np.ndarray, topn: int=10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate top-N recommendations based on the input scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores : np.ndarray\n",
    "        A dense numpy array of scores, where each row corresponds to a user and each column to an item.\n",
    "    topn : int, optional\n",
    "        The number of top recommendations to generate for each user. Defaults to 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A numpy array of shape (n_users, topn) containing the indices of the top-N recommended items for each user.\n",
    "    \"\"\"\n",
    "    recommendations = np.apply_along_axis(topidx, 1, scores, topn)\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def topidx(a: np.ndarray, topn: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the indices of the top-N elements in the input array.\n",
    "    The calculation is based on the argpartition method, which is more efficient for large arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : np.ndarray\n",
    "        The input array from which to select the top-N elements.\n",
    "    topn : int\n",
    "        The number of top elements to select.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array of indices corresponding to the top-N elements in the input array.\n",
    "    \"\"\"\n",
    "    parted = np.argpartition(a, -topn)[-topn:]\n",
    "    return parted[np.argsort(-a[parted])]\n",
    "\n",
    "\n",
    "def model_evaluate(\n",
    "        recommended_items: np.ndarray,\n",
    "        holdout: pd.DataFrame,\n",
    "        holdout_description: dict,\n",
    "        topn: int = 20\n",
    "    ) -> tuple:\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a recommender model by comparing the recommended items against the holdout.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    recommended_items : np.ndarray\n",
    "        A numpy array of shape (n_users, topn) containing the indices of the top-N recommended items for each user.\n",
    "    holdout : pd.DataFrame\n",
    "        A pandas DataFrame containing the holdout data for evaluation.\n",
    "    holdout_description : dict\n",
    "        A dictionary containing the description of the holdout data, including the column names for items.\n",
    "    topn : int, optional\n",
    "        The number of top recommendations to consider for evaluation. Defaults to 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        NDCG@k\n",
    "    \"\"\"\n",
    "    itemid = holdout_description['items']\n",
    "    holdout_items = holdout[itemid].values\n",
    "    assert recommended_items.shape[0] == len(holdout_items)\n",
    "\n",
    "    hits_mask = recommended_items[:, :topn] == holdout_items.reshape(-1, 1)\n",
    "    n_test_users = recommended_items.shape[0]\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.0\n",
    "    ndcg_pu = 1.0 / np.log2(hit_rank + 1)\n",
    "    ndcg = np.sum(ndcg_pu) / n_test_users\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def calculate_metrics(recommendations_df, test_df, user_col='userid', item_col='movieid', k=20):\n",
    "    \"\"\"\n",
    "    Calculates HR@k, MRR@k, Cov@k, and NDCG@k for the recommendations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    recommendations_df : pd.DataFrame\n",
    "        The recommendations DataFrame with columns 'userid' and 'movieid'.\n",
    "    test_df : pd.DataFrame\n",
    "        The test set DataFrame with columns 'userid' and 'movieid'.\n",
    "    user_col : str\n",
    "        The name of the user column.\n",
    "    item_col : str\n",
    "        The name of the item column.\n",
    "    k : int\n",
    "        The number of recommendations to consider (e.g., 20 for HR@20, MRR@20, Cov@20, NDCG@20).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[float, float, float, float]\n",
    "        The HR@k, MRR@k, Cov@k, and NDCG@k scores.\n",
    "    \"\"\"\n",
    "    ground_truth = test_df.groupby(user_col)[item_col].apply(set).to_dict()\n",
    "    \n",
    "    hits = 0\n",
    "    reciprocal_ranks = []\n",
    "    recommended_items = set()\n",
    "    total_ndcg = 0.0\n",
    "    \n",
    "    for user, recommendations in recommendations_df.groupby(user_col)[item_col]:\n",
    "        true_items = ground_truth.get(user, set())\n",
    "\n",
    "        recommended_items_list = recommendations.tolist()[:k]\n",
    "        \n",
    "        recommended_items.update(recommended_items_list)\n",
    "        \n",
    "        if any(item in true_items for item in recommended_items_list):\n",
    "            hits += 1\n",
    "        \n",
    "        for rank, item in enumerate(recommended_items_list, start=1):\n",
    "            if item in true_items:\n",
    "                reciprocal_ranks.append(1.0 / rank)\n",
    "                break\n",
    "        \n",
    "        dcg = 0.0\n",
    "        for i, item in enumerate(recommended_items_list):\n",
    "            if item in true_items:\n",
    "                dcg += 1.0 / np.log2(i + 2) \n",
    "        \n",
    "        idcg = 0.0\n",
    "        for i in range(min(len(true_items), k)):\n",
    "            idcg += 1.0 / np.log2(i + 2)\n",
    "        \n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        total_ndcg += ndcg\n",
    "    \n",
    "    # HR@k\n",
    "    hr = hits / len(ground_truth)\n",
    "    \n",
    "    # MRR@k\n",
    "    mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "    \n",
    "    # Cov@k\n",
    "    unique_items_in_test = set(test_df[item_col].unique())\n",
    "    cov = len(recommended_items) / len(unique_items_in_test)\n",
    "\n",
    "    # NDCG@k\n",
    "    ndcg = total_ndcg / len(ground_truth)\n",
    "    \n",
    "    return hr, mrr, cov, ndcg\n",
    "\n",
    "def recommendations_to_df(recommendations, test_users_ids, reverse_movie_id_map):\n",
    "    user_ids = []\n",
    "    movie_ids = []\n",
    "    \n",
    "    for user_idx, items in enumerate(recommendations):\n",
    "        user_id = test_users_ids[user_idx]\n",
    "        \n",
    "        remapped_items = [reverse_movie_id_map[item_idx] for item_idx in items]\n",
    "        \n",
    "        user_ids.extend([user_id] * len(remapped_items))\n",
    "        movie_ids.extend(remapped_items)\n",
    "    \n",
    "    return pd.DataFrame({'userid': user_ids, 'movieid': movie_ids})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a94d54-cb35-4fab-88d8-01f3e3c18640",
   "metadata": {},
   "source": [
    "Let's select only 4500 most watched films as they describe 95% of all interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51be6767-cbf5-4371-ac97-ef163aff9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = train_df['movieid'].value_counts().nlargest(7250).index\n",
    "\n",
    "train_filtered = train_df[train_df['movieid'].isin(top_movies)]\n",
    "\n",
    "data_description = dict(\n",
    "    users='userid',\n",
    "    items='movieid',\n",
    "    feedback='rating',\n",
    "    n_users=train_filtered.userid.nunique(),\n",
    "    n_items=train_filtered.movieid.nunique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "12afd530-feee-4234-a1a1-402264a261d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testset, holdout = timepoint_split(train_filtered, time_split_q=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb48e3dc-7e1d-4aef-b7f7-c6f527471dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, data_index = transform_indices(training, 'userid', 'movieid')\n",
    "testset = reindex_data(testset, data_index, entities='items')\n",
    "holdout = reindex_data(holdout, data_index, entities='items')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eee5a1-7a38-45ad-9543-53f007993b19",
   "metadata": {},
   "source": [
    "For recommendation generation we will use Trunctated SVD with decay over timestamp and scaling within users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "08cda92c-04c7-41ba-8e27-7216d3034b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GridSearch for the best alpha & rank:   0%|               | 0/9 [00:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 185\u001b[0m\n\u001b[1;32m    182\u001b[0m rank_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m501\u001b[39m, \u001b[38;5;241m50\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Run cross-validation\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m best_rank, best_alpha, best_decay_rate, best_metrics, results \u001b[38;5;241m=\u001b[39m cross_validate_svd(\n\u001b[1;32m    186\u001b[0m     training, testset, holdout, data_description, rank_values, alpha_values, decay_rates\n\u001b[1;32m    187\u001b[0m )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Best Rank: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Best Alpha: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_alpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Best Decay: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_decay_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ndcg@20: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_metrics[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[128], line 160\u001b[0m, in \u001b[0;36mcross_validate_svd\u001b[0;34m(training, testset, holdout, data_description, rank_values, alpha_values, decay_rates)\u001b[0m\n\u001b[1;32m    157\u001b[0m             scores \u001b[38;5;241m=\u001b[39m svd_scoring(P_test, Vt_rank, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    158\u001b[0m             recommendations \u001b[38;5;241m=\u001b[39m topn_recommendations(scores, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m             ndcg, ndcghr, mrr, coverage \u001b[38;5;241m=\u001b[39m model_evaluate(recommendations, holdout, data_description, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    161\u001b[0m             results[(rank, alpha, decay_rate)] \u001b[38;5;241m=\u001b[39m (hr, mrr, coverage, ndcg)\n\u001b[1;32m    163\u001b[0m best_rank, best_alpha, best_decay_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: results[x][\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Select based on HR@10\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[128], line 121\u001b[0m, in \u001b[0;36mmodel_evaluate\u001b[0;34m(recommended_items, holdout, holdout_description, topn)\u001b[0m\n\u001b[1;32m    119\u001b[0m itemid \u001b[38;5;241m=\u001b[39m holdout_description[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    120\u001b[0m holdout_items \u001b[38;5;241m=\u001b[39m holdout[itemid]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m recommended_items\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(holdout_items)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Hit Rate (HR@10)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m hits_mask \u001b[38;5;241m=\u001b[39m recommended_items[:, :topn] \u001b[38;5;241m==\u001b[39m holdout_items\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def scale_matrix(matrix, alpha=0.5):\n",
    "    \"\"\"Scales the interaction matrix using the alpha parameter.\"\"\"\n",
    "    item_interactions = matrix.getnnz(axis=0)\n",
    "    scaling_factors = np.power(item_interactions, (alpha - 1) / 2)\n",
    "    D = diags(scaling_factors)\n",
    "    return matrix.dot(D)\n",
    "\n",
    "def normalize_timestamps(timestamps, min_timestamp, max_timestamp):\n",
    "    \"\"\"Normalizes timestamps to a range of [0, 1].\"\"\"\n",
    "    return (timestamps - min_timestamp) / (max_timestamp - min_timestamp)\n",
    "\n",
    "def compute_decay_weights(normalized_timestamps, decay_rate):\n",
    "    \"\"\"Computes decay weights for interactions based on normalized timestamps.\"\"\"\n",
    "    return np.power(decay_rate, 1 - normalized_timestamps)\n",
    "\n",
    "def apply_decay_to_matrix(data, decay_rate, timestamp_col='timestamp', feedback_col='rating'):\n",
    "    \"\"\"Applies decay weights to the interaction matrix based on timestamps.\"\"\"\n",
    "    min_timestamp = data[timestamp_col].min()\n",
    "    max_timestamp = data[timestamp_col].max()\n",
    "    normalized_timestamps = normalize_timestamps(data[timestamp_col], min_timestamp, max_timestamp)\n",
    "    decay_weights = compute_decay_weights(normalized_timestamps, decay_rate)\n",
    "    decayed_feedback = data[feedback_col] * decay_weights\n",
    "    user_idx = data['userid'].values\n",
    "    item_idx = data['movieid'].values\n",
    "    return csr_matrix((decayed_feedback, (user_idx, item_idx)), shape=(data['userid'].nunique(), data['movieid'].nunique()))\n",
    "\n",
    "# ------------------------\n",
    "# 2️⃣ Training SVD Model\n",
    "# ------------------------\n",
    "def build_svd_model(training, rank, alpha, decay_rate):\n",
    "    \"\"\"Trains SVD model only on training data (no test users).\"\"\"\n",
    "    decayed_matrix = apply_decay_to_matrix(training, decay_rate)\n",
    "    scaled_matrix = scale_matrix(decayed_matrix, alpha)\n",
    "    U, sigma, Vt = svds(scaled_matrix, k=rank)\n",
    "    return U, np.diag(sigma), Vt\n",
    "\n",
    "# ------------------------\n",
    "# 3️⃣ Folding-in Test Users\n",
    "# ------------------------\n",
    "def fold_in_users(U, Vt, testset, data_description, n_epochs=10, lr=0.01, reg=0.1):\n",
    "    \"\"\"Infers test user embeddings using pre-trained item factors.\"\"\"\n",
    "\n",
    "    user_ids = testset[data_description['users']].values\n",
    "    item_idx = testset[data_description['items']].values\n",
    "    ratings = testset[data_description['feedback']].values\n",
    "\n",
    "    unique_users = np.unique(user_ids)\n",
    "    user_map = {uid: idx for idx, uid in enumerate(unique_users)}  # Map user ID → local index\n",
    "\n",
    "    n_users_test = len(unique_users)\n",
    "    rank = Vt.shape[0]\n",
    "\n",
    "    P_test = np.zeros((n_users_test, rank))  # Correct shape\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(len(user_ids)):\n",
    "            user = user_map[user_ids[i]]  # Convert user ID to test index\n",
    "            item = item_idx[i]\n",
    "            rating = ratings[i]\n",
    "\n",
    "            qi = Vt.T[item]  # Item factor (rank x 1)\n",
    "            pu = P_test[user]  # User factor (1 x rank)\n",
    "\n",
    "            err = rating - np.dot(pu, qi)  # Correct matrix multiplication\n",
    "            P_test[user] += lr * (err * qi - reg * pu)  # Update step\n",
    "\n",
    "    return P_test\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 4️⃣ Compute Scores\n",
    "# ------------------------\n",
    "def svd_scoring(P_test, Vt, topn=20):\n",
    "    \"\"\"Generates predicted scores for test users.\"\"\"\n",
    "    scores = P_test @ Vt\n",
    "    return scores\n",
    "\n",
    "# ------------------------\n",
    "# 5️⃣ Generate Recommendations\n",
    "# ------------------------\n",
    "def topn_recommendations(scores, topn=20):\n",
    "    \"\"\"Generates top-N recommendations.\"\"\"\n",
    "    return np.argsort(-scores, axis=1)[:, :topn]\n",
    "\n",
    "# ------------------------\n",
    "# 6️⃣ Evaluate Recommendations\n",
    "# ------------------------\n",
    "import numpy as np\n",
    "\n",
    "def dcg_at_k(ranked_relevances, k):\n",
    "    \"\"\"Computes Discounted Cumulative Gain (DCG) at k.\"\"\"\n",
    "    ranked_relevances = np.asarray(ranked_relevances)[:k]\n",
    "    return np.sum(ranked_relevances / np.log2(np.arange(2, k + 2)))\n",
    "\n",
    "def ndcg_at_k(recommended_items, holdout_items, k=20):\n",
    "    \"\"\"Computes Normalized DCG (NDCG) at k.\"\"\"\n",
    "    dcg = 0\n",
    "    idcg = dcg_at_k([1] * min(len(holdout_items), k), k)  # Ideal DCG\n",
    "\n",
    "    for i, user_recs in enumerate(recommended_items):\n",
    "        user_holdout = {holdout_items[i]}  # ✅ Convert single item to a set\n",
    "        relevance = [1 if item in user_holdout else 0 for item in user_recs]  # Binary relevance\n",
    "        dcg += dcg_at_k(relevance, k)\n",
    "\n",
    "    return (dcg / len(recommended_items)) / idcg if idcg > 0 else 0\n",
    "\n",
    "\n",
    "def model_evaluate(recommended_items, holdout, holdout_description, topn=20):\n",
    "    \"\"\"Evaluates the model using HR@10, MRR@10, Coverage, and NDCG@20.\"\"\"\n",
    "    itemid = holdout_description['items']\n",
    "    holdout_items = holdout[itemid].values\n",
    "    assert recommended_items.shape[0] == len(holdout_items)\n",
    "\n",
    "    # Hit Rate (HR@10)\n",
    "    hits_mask = recommended_items[:, :topn] == holdout_items.reshape(-1, 1)\n",
    "    hr = np.mean(hits_mask.any(axis=1))\n",
    "\n",
    "    # Mean Reciprocal Rank (MRR@10)\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.0\n",
    "    mrr = np.sum(1 / hit_rank) / recommended_items.shape[0]\n",
    "\n",
    "    # Coverage\n",
    "    n_items = holdout_description['n_items']\n",
    "    coverage = np.unique(recommended_items).size / n_items\n",
    "\n",
    "    # NDCG@20\n",
    "    ndcg = ndcg_at_k(recommended_items, holdout_items, k=20)\n",
    "\n",
    "    return ndcg, hr, mrr, coverage\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 7️⃣ Cross-Validation for Best Rank & Alpha\n",
    "# ------------------------\n",
    "def cross_validate_svd(training, testset, holdout, data_description, rank_values, alpha_values, decay_rates):\n",
    "    \"\"\"Performs cross-validation to find the best rank & alpha using SVD.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for decay_rate in decay_rates:\n",
    "        for alpha in alpha_values:\n",
    "            max_rank = max(rank_values)\n",
    "            _, _, Vt = build_svd_model(training, max_rank, alpha, decay_rate)\n",
    "    \n",
    "            for rank in tqdm(rank_values, desc='GridSearch for the best alpha & rank'):\n",
    "                Vt_rank = Vt[:rank, :]\n",
    "                P_test = fold_in_users(_, Vt_rank, testset, data_description)\n",
    "    \n",
    "                scores = svd_scoring(P_test, Vt_rank, topn=20)\n",
    "                recommendations = topn_recommendations(scores, topn=20)\n",
    "    \n",
    "                ndcg, ndcghr, mrr, coverage = model_evaluate(recommendations, holdout, data_description, topn=20)\n",
    "                results[(rank, alpha, decay_rate)] = (hr, mrr, coverage, ndcg)\n",
    "\n",
    "    best_rank, best_alpha, best_decay_rate = max(results, key=lambda x: results[x][0])  # Select based on HR@10\n",
    "    best_metrics = results[(best_rank, best_alpha, best_decay_rate)]\n",
    "    \n",
    "    return best_rank, best_alpha, best_decay_rate, best_metrics, results\n",
    "\n",
    "# ------------------------\n",
    "# 8️⃣ Running Everything\n",
    "# ------------------------\n",
    "data_description = {\n",
    "    \"users\": \"userid\",\n",
    "    \"items\": \"movieid\",\n",
    "    \"feedback\": \"rating\",\n",
    "    \"n_users\": training.userid.nunique(),\n",
    "    \"n_items\": training.movieid.nunique()\n",
    "}\n",
    "\n",
    "# Set hyperparameters\n",
    "decay_rates = [0.93, 0.95, 0.97, 0.99, 1.01, 1.03]\n",
    "alpha_values = [0.35, 0.45, 0.55]\n",
    "rank_values = np.arange(100, 501, 50).tolist()\n",
    "\n",
    "# Run cross-validation\n",
    "best_rank, best_alpha, best_decay_rate, best_metrics, results = cross_validate_svd(\n",
    "    training, testset, holdout, data_description, rank_values, alpha_values, decay_rates\n",
    ")\n",
    "\n",
    "print(f\"✅ Best Rank: {best_rank}, Best Alpha: {best_alpha}, Best Decay: {best_decay_rate}, ndcg@20: {best_metrics[0]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "983310b8-85f8-458a-8095-0e5761e2e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_matrix(matrix, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Scales the interaction matrix using the alpha parameter\n",
    "    \"\"\"\n",
    "    item_interactions = matrix.getnnz(axis=0) \n",
    "    \n",
    "    scaling_factors = np.power(item_interactions, (alpha - 1) / 2)\n",
    "    \n",
    "    D = diags(scaling_factors)\n",
    "    \n",
    "    scaled_matrix = matrix.dot(D)\n",
    "    \n",
    "    return scaled_matrix\n",
    "\n",
    "def build_svd_model(matrix, rank, alpha, decay_rate):\n",
    "    \"\"\"\n",
    "    Builds the SVD model for the given rank and alpha\n",
    "    \"\"\"\n",
    "    decayed_matrix = apply_decay_to_matrix(matrix, decay_rate)\n",
    "    scaled_matrix = scale_matrix(decayed_matrix, alpha)\n",
    "    U, sigma, Vt = svds(scaled_matrix, k=rank)\n",
    "    sigma = np.diag(sigma)\n",
    "    return U, sigma, Vt\n",
    "\n",
    "def cross_validate_svd(training, data_description, holdout, rank_values, alpha_values, decay_rate):\n",
    "    \"\"\"\n",
    "    Performs cross-validation to find the best rank and alpha using SVD\n",
    "    Builds the SVD model once for the maximum rank and shrinks it for smaller ranks\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    #train_matrix = generate_interactions_matrix(training, data_description)\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        \n",
    "        max_rank = max(rank_values)\n",
    "        *_, Vt = build_svd_model(training, max_rank, alpha, decay_rate)\n",
    "        \n",
    "        for rank in tqdm(rank_values, desc='GridSearching for the best alpha and rank'):\n",
    "            Vt_rank = Vt[:rank, :]\n",
    "            \n",
    "            item_factors = Vt_rank.T\n",
    "            \n",
    "            scores = svd_scoring(item_factors, holdout, data_description)\n",
    "            \n",
    "            recommendations = topn_recommendations(scores, topn=20)\n",
    "            \n",
    "            ndcg = model_evaluate(recommendations, holdout, data_description, topn=20)\n",
    "            \n",
    "            results[(rank, alpha)] = ndcg\n",
    "    \n",
    "    best_rank, best_alpha = max(results, key=results.get)\n",
    "    best_ndcg = results[(best_rank, best_alpha)]\n",
    "    \n",
    "    return best_rank, best_alpha, best_ndcg, results\n",
    "\n",
    "def svd_scoring(item_factors, holdout, data_description):\n",
    "    \"\"\"\n",
    "    Generates scores for the holdout set using the item factors\n",
    "    \"\"\"\n",
    "    test_matrix = generate_interactions_matrix(holdout, data_description, rebase_users=True)\n",
    "    print(test_matrix.shape, item_factors.shape)\n",
    "    scores = test_matrix.dot(item_factors) @ item_factors.T\n",
    "    return scores\n",
    "\n",
    "def recommendations_to_df(recommendations, test_users_ids, reverse_movie_id_map):\n",
    "    user_ids = []\n",
    "    movie_ids = []\n",
    "    \n",
    "    for user_idx, items in enumerate(recommendations):\n",
    "        user_id = test_users_ids[user_idx]\n",
    "        \n",
    "        remapped_items = [reverse_movie_id_map[item_idx] for item_idx in items]\n",
    "        \n",
    "        user_ids.extend([user_id] * len(remapped_items))\n",
    "        movie_ids.extend(remapped_items)\n",
    "    \n",
    "    return pd.DataFrame({'userid': user_ids, 'movieid': movie_ids})\n",
    "\n",
    "\n",
    "def normalize_timestamps(timestamps, min_timestamp, max_timestamp):\n",
    "    \"\"\"\n",
    "    Normalizes timestamps to a range of [0, 1]\n",
    "    \"\"\"\n",
    "    return (timestamps - min_timestamp) / (max_timestamp - min_timestamp)\n",
    "\n",
    "def compute_decay_weights(normalized_timestamps, decay_rate):\n",
    "    \"\"\"\n",
    "    Computes decay weights for interactions based on normalized timestamps\n",
    "    \"\"\"\n",
    "    return np.power(decay_rate, 1 - normalized_timestamps)\n",
    "\n",
    "def apply_decay_to_matrix(data, decay_rate, timestamp_col='timestamp', feedback_col='rating'):\n",
    "    \"\"\"\n",
    "    Applies decay weights to the interaction matrix based on timestamps\n",
    "    \"\"\"\n",
    "    min_timestamp = data[timestamp_col].min()\n",
    "    max_timestamp = data[timestamp_col].max()\n",
    "    normalized_timestamps = (data[timestamp_col] - min_timestamp) / (max_timestamp - min_timestamp) \n",
    "    \n",
    "    decay_weights = compute_decay_weights(normalized_timestamps, decay_rate)\n",
    "    \n",
    "    decayed_feedback = data[feedback_col] * decay_weights\n",
    "    \n",
    "    user_idx = data['userid'].values\n",
    "    item_idx = data['movieid'].values\n",
    "    return csr_matrix((decayed_feedback, (user_idx, item_idx)), shape=(data['userid'].nunique(), data['movieid'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e714c-f21b-4140-8fe4-28faba8ff4e8",
   "metadata": {},
   "source": [
    "### Cross-validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e187bf9-cf72-4506-b7f5-ee38a746d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_description = dict(\n",
    "#     users='userid',\n",
    "#     items='movieid',\n",
    "#     feedback='rating',\n",
    "#     n_users=train_filtered.userid.nunique(),\n",
    "#     n_items=train_filtered.movieid.nunique()\n",
    "# )\n",
    "\n",
    "# decay_rate = 0.93\n",
    "# alpha_values = [0.35]\n",
    "# rank_values = np.arange(130, 301, 5).tolist()\n",
    "\n",
    "# best_rank, best_alpha, best_ndcg, results = cross_validate_svd(\n",
    "#     training, data_description, holdout, rank_values, alpha_values, decay_rate\n",
    "# )\n",
    "\n",
    "# print(f\"Best rank: {best_rank} with NDCG@20: {best_ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af5218-a3a8-4440-8b19-83f9b42e03a3",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "93fd7aee-0fea-44ae-8d37-6411fa51326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = train_df['movieid'].value_counts().nlargest(7250).index\n",
    "\n",
    "train_filtered = train_df[train_df['movieid'].isin(top_movies)]\n",
    "\n",
    "training, data_index = transform_indices(train_filtered, 'userid', 'movieid')\n",
    "testset = reindex_data(test_df, data_index, entities='items')\n",
    "\n",
    "# Step 4: Define the data description dictionary\n",
    "data_description = dict(\n",
    "    users='userid',\n",
    "    items='movieid',\n",
    "    feedback='rating',\n",
    "    n_users=training.userid.nunique(),\n",
    "    n_items=training.movieid.nunique()\n",
    ")\n",
    "\n",
    "_, _, Vt = build_svd_model(training, rank=200, alpha=0.35, decay_rate=1.05)\n",
    "\n",
    "P_test = fold_in_users(_, Vt, testset, data_description)\n",
    "\n",
    "scores = svd_scoring(P_test, Vt, topn=20)\n",
    "#scores = svd_scoring(item_factors, testset, data_description)\n",
    "\n",
    "# Step 8: Downvote previously seen items to prevent re-recommendation\n",
    "downvote_seen_items(scores, testset, data_description)\n",
    "\n",
    "# Step 9: Generate top-20 recommendations for test users\n",
    "recommendations = topn_recommendations(scores, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "35a6aaf0-0fe6-4655-b511-13093c4688c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_movies = train_df['movieid'].value_counts().nlargest(7250).index\n",
    "\n",
    "# train_filtered = train_df[train_df['movieid'].isin(top_movies)]\n",
    "\n",
    "# training, data_index = transform_indices(train_filtered, 'userid', 'movieid')\n",
    "# testset = reindex_data(test_df, data_index, entities='items')\n",
    "\n",
    "# data_description = dict(\n",
    "#     users='userid',\n",
    "#     items='movieid',\n",
    "#     feedback='rating',\n",
    "#     n_users=training.userid.nunique(),\n",
    "#     n_items=training.movieid.nunique()\n",
    "# )\n",
    "\n",
    "# U, sigma, Vt = build_svd_model(training, rank=250, alpha=0.35, decay_rate = 0.93)\n",
    "# item_factors = Vt.T\n",
    "\n",
    "# scores = svd_scoring(item_factors, testset, data_description)\n",
    "# downvote_seen_items(scores, testset, data_description)\n",
    "\n",
    "# recommendations = topn_recommendations(scores, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f160df6b-f3c3-4708-9bb9-0f99db9e2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_user_id_map = {new: old for new, old in zip(range(len(data_index['users'])), data_index['users'])}\n",
    "reverse_movie_id_map = {new: old for new, old in zip(range(len(data_index['items'])), data_index['items'])}\n",
    "\n",
    "test_users_ids = pd.Index(testset['userid'].drop_duplicates())\n",
    "\n",
    "recommendations_df = recommendations_to_df(recommendations, test_users_ids, reverse_movie_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a02bdc92-b4f9-4087-9f1f-f8a005338c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@20: 0.886\n",
      "MRR@20: 0.475\n",
      "Cov@20: 0.345\n",
      "NDCG@20: 0.208\n"
     ]
    }
   ],
   "source": [
    "hr, mrr, cov, ndcg = calculate_metrics(recommendations_df, testset, k=20)\n",
    "print(f\"HR@20: {hr:.3}\")\n",
    "print(f\"MRR@20: {mrr:.3}\")\n",
    "print(f\"Cov@20: {cov:.3}\")\n",
    "print(f\"NDCG@20: {ndcg:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "45fcb01e-33ca-4ee0-94ff-6475c5e9c3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@20: 0.889\n",
      "MRR@20: 0.476\n",
      "Cov@20: 0.334\n",
      "NDCG@20: 0.211\n"
     ]
    }
   ],
   "source": [
    "hr, mrr, cov, ndcg = calculate_metrics(recommendations_df, testset, k=20)\n",
    "print(f\"HR@20: {hr:.3}\")\n",
    "print(f\"MRR@20: {mrr:.3}\")\n",
    "print(f\"Cov@20: {cov:.3}\")\n",
    "print(f\"NDCG@20: {ndcg:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c261b-fec7-45a9-b9f1-ccba1ec46116",
   "metadata": {},
   "source": [
    "Hit Rate is quite good: we correctly guess 18/20 what to watch, Mean Reciprocical Rate tells us that on average we recommend good item on the 2nd position - good enough too! Coverage is ok - it shows us that our recommendation contain almost 1/3 of all different movies. NDCG helps consider both the relevance of items and their positions in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9216472-adb6-4501-9de5-5950274f2d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>movieid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>1586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>55</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>55</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>55</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>55</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>55</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>55</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>55</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>55</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>55</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>55</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>55</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>133</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userid  movieid\n",
       "0       55      194\n",
       "1       55      222\n",
       "2       55      755\n",
       "3       55      750\n",
       "4       55      110\n",
       "5       55     1586\n",
       "6       55     1411\n",
       "7       55      103\n",
       "8       55      120\n",
       "9       55      213\n",
       "10      55      206\n",
       "11      55      885\n",
       "12      55     1528\n",
       "13      55      631\n",
       "14      55      235\n",
       "15      55      391\n",
       "16      55       48\n",
       "17      55      972\n",
       "18      55     1084\n",
       "19      55      422\n",
       "20     133      174"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations_df.head(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a39f842e-eeb7-4eb1-a9e4-6d604b289888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>movieid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>55</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>55</td>\n",
       "      <td>1586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>55</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>55</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55</td>\n",
       "      <td>791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>55</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>55</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>55</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>55</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>55</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>55</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>55</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>133</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userid  movieid\n",
       "0       55      194\n",
       "1       55      222\n",
       "2       55      755\n",
       "3       55      750\n",
       "4       55     1411\n",
       "5       55      110\n",
       "6       55     1528\n",
       "7       55     1586\n",
       "8       55      213\n",
       "9       55      120\n",
       "10      55      972\n",
       "11      55      103\n",
       "12      55      791\n",
       "13      55      529\n",
       "14      55      391\n",
       "15      55     1084\n",
       "16      55      619\n",
       "17      55      631\n",
       "18      55     1400\n",
       "19      55     1096\n",
       "20     133      981"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations_df.head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0815bb-10f1-45ba-ac4c-5b789abb73bd",
   "metadata": {},
   "source": [
    "## Final submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c69084b1-42bc-4578-b8ff-e89ae59099f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
